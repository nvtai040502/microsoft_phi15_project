{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotary Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "from einops import rearrange # einstein operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "sample = 100\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # You can choose any appropriate token for padding\n",
    "\n",
    "subset_dataset = dataset['train'][:sample]['text']\n",
    "tokenized_dataset = tokenizer(\n",
    "    subset_dataset,\n",
    "    return_tensors='pt',\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True  # Enable truncation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "data = tokenized_dataset['input_ids']\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InferenceParams\n",
    "class InferenceParams(nn.Module):\n",
    "    def __init__(self, sequence_len):\n",
    "        \n",
    "        self.rotary_dim = 3\n",
    "        self.n_layer = 2\n",
    "        \n",
    "        self.sequence_len = sequence_len\n",
    "        self.batch_size = 16\n",
    "        self.n_embd = 20\n",
    "        self.n_head = 4\n",
    "        self.vocab_size = 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = data.size(1) - 1\n",
    "config = InferenceParams(sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 297]), torch.Size([16, 297]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(data, batch_size):\n",
    "    idx = torch.randint(0, len(data), size=(batch_size,))\n",
    "    batch = data[idx]\n",
    "\n",
    "    xb = batch[:, :-1].contiguous()\n",
    "    yb = batch[:, 1:].contiguous()\n",
    "    \n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch(data, config.batch_size)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        hidden_states = self.wte(input_ids)\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297, 20])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Embedding(config)\n",
    "hidden_states = m(xb)\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionEmbedding(nn.Module):\n",
    "    def __init__(self, config, base = 10000):\n",
    "        super().__init__()\n",
    "        self.rotary_dim  = config.rotary_dim \n",
    "        \n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.rotary_dim, 2) / self.rotary_dim ))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        self.cos_cache = None\n",
    "        self.sin_cache = None\n",
    "        \n",
    "    def forward(self, qkv):\n",
    "        seqlen = qkv.shape[1]\n",
    "        \n",
    "        # Update cos sin cache\n",
    "        t = torch.arange(seqlen)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        \n",
    "        self.cos_cache = torch.cos(freqs)\n",
    "        self.sin_cache = torch.sin(freqs)\n",
    "        \n",
    "        # Apply rotary qkv\n",
    "        rotary_dim = self.cos_cache.shape[1]\n",
    "        rotary_dim *= 2\n",
    "        \n",
    "        q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "        q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "        \n",
    "        k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "        k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "        \n",
    "        # Splits the queries and keys in half\n",
    "        q1, q2 = q_rot.chunk(2, dim=-1)\n",
    "        k1, k2 = k_rot.chunk(2, dim=-1)\n",
    "        c, s = rearrange(self.cos_cache, \"t d -> t 1 d\"), rearrange(self.sin_cache, \"t d -> t 1 d\")\n",
    "        \n",
    "        # Computes the new keys and queries\n",
    "        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n",
    "        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n",
    "        \n",
    "        return torch.cat(\n",
    "            [\n",
    "                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n",
    "                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n",
    "                qkv[:, :, 2:3, :, :]\n",
    "            ],\n",
    "            dim=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        n_inner = 4 * config.n_embd\n",
    "        \n",
    "        self.fc1 = nn.Linear(config.n_embd, n_inner)\n",
    "        self.fc2 = nn.Linear(n_inner, config.n_embd)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297, 20])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MLP(config)\n",
    "ffwd_out = m(hidden_states)\n",
    "ffwd_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self, qkv):\n",
    "        seq_len = qkv.shape[1]\n",
    "        q, k, v = qkv.unbind(2)\n",
    "        \n",
    "        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n",
    "        scores = torch.einsum(\"bthd, bshd -> bhts\", q, k * softmax_scale)\n",
    "        \n",
    "        mask = torch.triu(torch.full((seq_len, seq_len), -10000), 1)\n",
    "        scores += mask\n",
    "        \n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        output = torch.einsum(\"bhts, bshd -> bthd\", attention, v)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rotary_emb = RotaryPositionEmbedding(config)\n",
    "        \n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        opt_size = config.n_head * self.head_dim\n",
    "        hidden_size = config.n_embd\n",
    "        \n",
    "        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n",
    "        self.out_proj = nn.Linear(opt_size, hidden_size)\n",
    "        \n",
    "        self.inner_attn = SelfAttention()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        qkv = self.Wqkv(x)\n",
    "        qkv = rearrange(qkv, 'b t (three h d) -> b t three h d', three=3, d=self.head_dim)\n",
    "        \n",
    "        qkv = self.rotary_emb(qkv)\n",
    "        \n",
    "        output = self.inner_attn(qkv)\n",
    "        \n",
    "        output = rearrange(output, \"... h d -> ... (h d)\")\n",
    "        attn_out = self.out_proj(output)\n",
    "        \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297, 20])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MHA(config)\n",
    "attn_out = m(hidden_states)\n",
    "attn_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln = nn.LayerNorm(config.n_embd)\n",
    "        \n",
    "        self.attn = MHA(config)\n",
    "        self.ffwd = MLP(config)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        \n",
    "        attn_out = self.attn(hidden_states)\n",
    "        \n",
    "        ffwd_out = self.ffwd(hidden_states)\n",
    "        \n",
    "        output = attn_out + ffwd_out + residual\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297, 20])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Block(config)\n",
    "output = m(hidden_states)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln = nn.LayerNorm(config.n_embd)\n",
    "        self.linear = nn.Linear(config.n_embd, config.vocab_size)\n",
    "        \n",
    "    def forward(self, output):\n",
    "        output = self.ln(output)\n",
    "        logits = self.linear(output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297, 50257])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LMHead(config)\n",
    "logits = m(output)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialForLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_layers = 2\n",
    "        \n",
    "        modules = [Embedding(config)]\n",
    "        modules += [Block(config) for _ in range(n_layers)]\n",
    "        modules.append(LMHead(config))\n",
    "        \n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        logits = self.layers(input_ids)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297, 50257])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = SequentialForLM(config)\n",
    "logits = m(xb)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        labels = labels.view(-1)                    \n",
    "                             \n",
    "        loss = self.loss_fct(logits, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.1403, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_loss = LMLoss()\n",
    "loss = lm_loss(logits, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8916, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = InferenceParams(sequence_len)\n",
    "xb, yb = get_batch(data, config.batch_size)\n",
    "\n",
    "m = SequentialForLM(config)\n",
    "logits = m(xb)\n",
    "\n",
    "lm_loss = LMLoss()\n",
    "loss = lm_loss(logits, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
