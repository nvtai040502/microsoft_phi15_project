{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "from einops import rearrange # einstein operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "sample = 100\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # You can choose any appropriate token for padding\n",
    "\n",
    "subset_dataset = dataset['train'][:sample]['text']\n",
    "tokenized_dataset = tokenizer(\n",
    "    subset_dataset,\n",
    "    return_tensors='pt',\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True  # Enable truncation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 298])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tokenized_dataset['input_ids']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InferenceParams\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "n_head = 4\n",
    "n_embd = 36\n",
    "sequence_len = data.size(1) - 1\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 297]), torch.Size([16, 297]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(data, batch_size):\n",
    "    idx = torch.randint(0, len(data), size=(batch_size,))\n",
    "    batch = data[idx]\n",
    "\n",
    "    xb = batch[:, :-1].contiguous()\n",
    "    yb = batch[:, 1:].contiguous()\n",
    "    \n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch(data, batch_size)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, sequence_len):\n",
    "        super().__init__()\n",
    "        self.sequence_len = sequence_len\n",
    "        \n",
    "        self.wte = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position = nn.Embedding(sequence_len, n_embd)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        token_embd = self.wte(input_ids)\n",
    "        position_embd = self.position(torch.arange(self.sequence_len))\n",
    "        \n",
    "        hidden_states = token_embd + position_embd        \n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297, 36])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Embedding(vocab_size, n_embd, sequence_len)\n",
    "hidden_states = m(xb)\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        n_inner = 4 * n_embd\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_embd, n_inner)\n",
    "        self.fc2 = nn.Linear(n_inner, n_embd)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297, 36])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MLP(n_embd)\n",
    "ffwd_out = m(hidden_states)\n",
    "ffwd_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self, qkv):\n",
    "        seq_len = qkv.shape[1]\n",
    "        q, k, v = qkv.unbind(2)\n",
    "        \n",
    "        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n",
    "        scores = torch.einsum(\"bthd, bshd -> bhts\", q, k * softmax_scale)\n",
    "        \n",
    "        mask = torch.triu(torch.full((seq_len, seq_len), -10000), 1)\n",
    "        scores += mask\n",
    "        \n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        output = torch.einsum(\"bhts, bshd -> bthd\", attention, v)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.head_dim = n_embd // n_head\n",
    "        opt_size = n_head * self.head_dim\n",
    "        hidden_size = n_embd\n",
    "        \n",
    "        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n",
    "        self.out_proj = nn.Linear(opt_size, hidden_size)\n",
    "        \n",
    "        self.inner_attn = SelfAttention()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        qkv = self.Wqkv(x)\n",
    "        qkv = rearrange(qkv, 'b t (three h d) -> b t three h d', three=3, d=self.head_dim)\n",
    "        \n",
    "        output = self.inner_attn(qkv)\n",
    "        \n",
    "        output = rearrange(output, \"... h d -> ... (h d)\")\n",
    "        attn_out = self.out_proj(output)\n",
    "        \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297, 36])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MHA(n_embd, n_head)\n",
    "attn_out = m(hidden_states)\n",
    "attn_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        self.ffwd = MLP(n_embd)\n",
    "        self.attn = MHA(n_embd, n_head)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        \n",
    "        attn_out = self.attn(hidden_states)\n",
    "        \n",
    "        ffwd_out = self.ffwd(hidden_states)\n",
    "        \n",
    "        output = attn_out + ffwd_out + residual\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297, 36])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Block(n_embd, n_head)\n",
    "output = m(hidden_states)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMHead(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        self.linear = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, output):\n",
    "        output = self.ln(output)\n",
    "        logits = self.linear(output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297, 50257])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LMHead(vocab_size, n_embd)\n",
    "logits = m(output)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels):              \n",
    "                             \n",
    "        loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.1934, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_loss = LMLoss()\n",
    "loss = lm_loss(logits, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
