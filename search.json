[
  {
    "objectID": "code5.html",
    "href": "code5.html",
    "title": "Training Model",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfrom einops import rearrange # einstein operation\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice\n\ndevice(type='cuda', index=0)"
  },
  {
    "objectID": "code5.html#download-dataset",
    "href": "code5.html#download-dataset",
    "title": "Training Model",
    "section": "Download Dataset",
    "text": "Download Dataset\n\nclass InferenceParams(nn.Module):\n    def __init__(self):\n\n        self.n_epochs = 5\n        self.learning_rate = .01\n        self.initializer_range = .02\n\n        self.max_sequence_len = 256\n        self.rotary_dim = 3\n        self.n_layer = 4\n        self.batch_size = 32\n        self.n_embd = 20\n        self.n_head = 4\n        self.vocab_size = 50257\n\nconfig = InferenceParams()\n\n\nsample = 10000\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token\n\ntrain_subset = dataset['train'][:sample]['text']\nval_subset = dataset['validation'][:sample]['text']\n\ntokenized_trainset = tokenizer(\n    train_subset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=config.max_sequence_len  # Set the maximum sequence length\n)\n\ntokenized_valset = tokenizer(\n    val_subset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=config.max_sequence_len  # Set the maximum sequence length\n)\n\nRepo card metadata block was not found. Setting CardData to empty.\nWARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n\n\n\nfor i in range(5):\n    print(f\"{tokenized_valset['input_ids'][i, -3:]} --&gt; {tokenized_valset['attention_mask'][i,-3:]}\")\n    print(\"\")\n\ntensor([50256, 50256, 50256]) --&gt; tensor([0, 0, 0])\n\ntensor([  319,   262, 30284]) --&gt; tensor([1, 1, 1])\n\ntensor([50256, 50256, 50256]) --&gt; tensor([0, 0, 0])\n\ntensor([50256, 50256, 50256]) --&gt; tensor([0, 0, 0])\n\ntensor([50256, 50256, 50256]) --&gt; tensor([0, 0, 0])\n\n\n\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.data = tokenized_data\n\n    def __len__(self):\n        return len(self.data['input_ids'])\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.data['input_ids'][idx],\n            'attention_mask': self.data['attention_mask'][idx]\n        }\n\ntrain_data = CustomDataset(tokenized_trainset)\nval_data = CustomDataset(tokenized_valset)\n\ntrain_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=config.batch_size)\n\n\nbatch = next(iter(train_loader))\ninput_ids = batch['input_ids']\nattention_mask = batch['attention_mask']\n\nprint(input_ids.shape, attention_mask.shape)\n\ntorch.Size([32, 256]) torch.Size([32, 256])"
  },
  {
    "objectID": "code5.html#embedding",
    "href": "code5.html#embedding",
    "title": "Training Model",
    "section": "Embedding",
    "text": "Embedding\n\nclass Embedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n\n    def forward(self, input_ids):\n        input_shape = input_ids.shape[-1]\n        input_ids = input_ids.view(-1, input_shape)\n\n        hidden_states = self.wte(input_ids)\n\n        return hidden_states\n\n\nclass RotaryPositionEmbedding(nn.Module):\n    def __init__(self, config, base = 10000):\n        super().__init__()\n        self.dim = config.rotary_dim\n\n        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        self.cos_cache = None\n        self.sin_cache = None\n\n    def forward(self, qkv):\n        seqlen = qkv.shape[1]\n\n        # Update cos sin cache\n        t = torch.arange(seqlen, device = qkv.device)\n        freqs = torch.outer(t, self.inv_freq)\n\n        self.cos_cache = torch.cos(freqs)\n        self.sin_cache = torch.sin(freqs)\n\n        # Apply rotary qkv\n        rotary_dim = self.cos_cache.shape[1]\n        rotary_dim *= 2\n\n        q_rot = qkv[:, :, 0, :, :rotary_dim]\n        q_pass = qkv[:, :, 0, :, rotary_dim:]\n\n        k_rot = qkv[:, :, 1, :, :rotary_dim]\n        k_pass = qkv[:, :, 1, :, rotary_dim:]\n\n        # Splits the queries and keys in half\n        q1, q2 = q_rot.chunk(2, dim=-1)\n        k1, k2 = k_rot.chunk(2, dim=-1)\n        c, s = rearrange(self.cos_cache, \"t d -&gt; t 1 d\"), rearrange(self.sin_cache, \"t d -&gt; t 1 d\")\n\n        # Computes the new keys and queries\n        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n\n        return torch.cat(\n            [\n                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n                qkv[:, :, 2:3, :, :]\n            ],\n            dim=2\n        )"
  },
  {
    "objectID": "code5.html#block",
    "href": "code5.html#block",
    "title": "Training Model",
    "section": "Block",
    "text": "Block\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        n_inner = 4 * config.n_embd\n\n        self.fc1 = nn.Linear(config.n_embd, n_inner)\n        self.fc2 = nn.Linear(n_inner, config.n_embd)\n        self.act = nn.ReLU()\n\n    def forward(self, hidden_states):\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n\n        return hidden_states\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        pass\n\n    def forward(self, qkv, attention_mask = None):\n        batch_size, seq_len = qkv.shape[0], qkv.shape[1]\n        q, k, v = qkv.unbind(2)\n\n        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n        scores = torch.einsum(\"bthd, bshd -&gt; bhts\", q, k * softmax_scale)\n\n        if attention_mask is not None:\n            padding_mask = torch.full((batch_size, seq_len), -10000.0, device=scores.device)\n            padding_mask.masked_fill_(attention_mask, 0.0)\n\n            scores = scores + rearrange(padding_mask, \"b s -&gt; b 1 1 s\")\n\n        casual_mask = torch.triu(torch.full((seq_len, seq_len), -10000, device=scores.device), 1)\n        scores += casual_mask\n\n        attention = torch.softmax(scores, dim=-1)\n\n        output = torch.einsum(\"bhts, bshd -&gt; bthd\", attention, v)\n\n        return output\n\n\nclass MHA(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.rotary_emb = RotaryPositionEmbedding(config)\n\n        self.head_dim = config.n_embd // config.n_head\n        opt_size = config.n_head * self.head_dim\n        hidden_size = config.n_embd\n\n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n\n        self.inner_attn = SelfAttention()\n\n    def forward(self, x, attention_mask = None):\n        qkv = self.Wqkv(x)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n\n        qkv = self.rotary_emb(qkv)\n\n        if attention_mask is not None:\n            attention_mask = attention_mask.bool().to(qkv.device)\n\n        output = self.inner_attn(qkv, attention_mask)\n\n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n        attn_out = self.out_proj(output)\n\n        return attn_out\n\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.ln = nn.LayerNorm(config.n_embd)\n\n        self.attn = MHA(config)\n        self.ffwd = MLP(config)\n\n    def forward(self, hidden_states, attention_mask = None):\n        residual = hidden_states\n        hidden_states = self.ln(hidden_states)\n\n        attn_out = self.attn(hidden_states, attention_mask)\n        ffwd_out = self.ffwd(hidden_states)\n\n        output = attn_out + ffwd_out + residual\n        return output"
  },
  {
    "objectID": "code5.html#sequential",
    "href": "code5.html#sequential",
    "title": "Training Model",
    "section": "Sequential",
    "text": "Sequential\n\nclass LMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.ln = nn.LayerNorm(config.n_embd)\n        self.linear = nn.Linear(config.n_embd, config.vocab_size)\n\n    def forward(self, output):\n        output = self.ln(output)\n        logits = self.linear(output)\n\n        return logits\n\n\nclass SequentialForLM(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.initializer_range = config.initializer_range\n\n        modules = [Embedding(config)]\n        modules += [Block(config) for _ in range(config.n_layer)]\n        modules.append(LMHead(config))\n\n        self.layers = nn.Sequential(*modules)\n\n        self.apply(self._init_weights)\n\n    def forward(self, input_ids, attention_mask = None):\n        if attention_mask is not None and self.training:\n            print(\"`attention_mask` is not supported during training. Using it might lead to unexpected results.\")\n\n        if attention_mask is None:\n            logits = self.layers(input_ids)\n        else:\n            hidden_layer = self.layers[0](input_ids)\n            for module in self.layers[1:-1]:\n                hidden_layer = module(hidden_layer, attention_mask=attention_mask)\n            logits = self.layers[-1](hidden_layer)\n\n        return logits\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n\nclass LMLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.loss_fct = nn.CrossEntropyLoss()\n\n    def forward(self, logits, labels):\n\n        logits = logits[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n\n        loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        return loss"
  },
  {
    "objectID": "code5.html#train-model",
    "href": "code5.html#train-model",
    "title": "Training Model",
    "section": "Train Model",
    "text": "Train Model\n\nimport pytorch_lightning as pl\n\nclass ModelForVisualization(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n\n        self.learning_rate = .01\n\n        self.model = SequentialForLM(config)\n        self.loss = LMLoss()\n\n    def forward(self, input_ids, attention_mask = None):\n        return self.model(input_ids, attention_mask)\n\n    def training_step(self, batch, batch_idx):\n        input_ids = batch['input_ids']\n        # attention_mask = batch['attention_mask']\n\n        logits = self(input_ids)\n        loss = self.loss(logits, input_ids)\n\n        self.log(\"train loss\", loss, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n\n        logits = self(input_ids, attention_mask)\n        loss = self.loss(logits, input_ids)\n\n        self.log(\"valid loss\", loss, prog_bar=True)\n\n        return loss\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            lr=self.learning_rate,\n        )\n\n        return optimizer\n\n\nn_epochs = 5\n\nmodel = ModelForVisualization(config)\n\nlogger = pl.loggers.TensorBoardLogger('logs/')\n\ntrainer = pl.Trainer(max_epochs=n_epochs, logger=logger)\n\ntrainer.fit(model, train_loader, val_loader)\n\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name  | Type            | Params\n------------------------------------------\n0 | model | SequentialForLM | 2.1 M \n1 | loss  | LMLoss          | 0     \n------------------------------------------\n2.1 M     Trainable params\n0         Non-trainable params\n2.1 M     Total params\n8.323     Total estimated model params size (MB)\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# %reload_ext tensorboard\n# %tensorboard --logdir logs --port 6007"
  },
  {
    "objectID": "code7.html",
    "href": "code7.html",
    "title": "Training in large data",
    "section": "",
    "text": "!pip install transformers datasets einops pytorch_lightning tensorboard\n\nCollecting transformers\n  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 65.3 MB/s eta 0:00:00\nCollecting datasets\n  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 49.7 MB/s eta 0:00:00\nCollecting einops\n  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.6/44.6 kB 5.9 MB/s eta 0:00:00\nCollecting pytorch_lightning\n  Downloading pytorch_lightning-2.0.9.post0-py3-none-any.whl (727 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 727.7/727.7 kB 55.1 MB/s eta 0:00:00\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.13.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\nCollecting huggingface-hub&lt;1.0,&gt;=0.16.4 (from transformers)\n  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 31.8 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\nCollecting tokenizers&lt;0.15,&gt;=0.14 (from transformers)\n  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 89.1 MB/s eta 0:00:00\nCollecting safetensors&gt;=0.3.1 (from transformers)\n  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 74.4 MB/s eta 0:00:00\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\nRequirement already satisfied: pyarrow&gt;=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\nCollecting dill&lt;0.3.8,&gt;=0.3.0 (from datasets)\n  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 12.5 MB/s eta 0:00:00\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 24.0 MB/s eta 0:00:00\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 15.5 MB/s eta 0:00:00\nRequirement already satisfied: fsspec[http]&lt;2023.9.0,&gt;=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\nRequirement already satisfied: torch&gt;=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.0.1+cu118)\nCollecting torchmetrics&gt;=0.7.0 (from pytorch_lightning)\n  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 805.2/805.2 kB 68.6 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.5.0)\nCollecting lightning-utilities&gt;=0.7.0 (from pytorch_lightning)\n  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\nRequirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio&gt;=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.58.0)\nRequirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\nRequirement already satisfied: google-auth-oauthlib&lt;1.1,&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\nRequirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.4)\nRequirement already satisfied: protobuf&gt;=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.1)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.7)\nRequirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.41.2)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer&lt;4.0,&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (3.2.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (6.0.4)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (4.0.3)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.9.2)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.3.1)\nRequirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard) (5.3.1)\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard) (0.3.0)\nRequirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard) (1.16.0)\nRequirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib&lt;1.1,&gt;=0.5-&gt;tensorboard) (1.3.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2.0.5)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2023.7.22)\nCollecting huggingface-hub&lt;1.0,&gt;=0.16.4 (from transformers)\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 28.0 MB/s eta 0:00:00\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.11.0-&gt;pytorch_lightning) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.11.0-&gt;pytorch_lightning) (3.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.11.0-&gt;pytorch_lightning) (3.1.2)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.11.0-&gt;pytorch_lightning) (2.0.0)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch&gt;=1.11.0-&gt;pytorch_lightning) (3.27.5)\nRequirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch&gt;=1.11.0-&gt;pytorch_lightning) (17.0.1)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard) (2.1.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2023.3.post1)\nRequirement already satisfied: pyasn1&lt;0.6.0,&gt;=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard) (0.5.0)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;1.1,&gt;=0.5-&gt;tensorboard) (3.2.2)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.11.0-&gt;pytorch_lightning) (1.3.0)\nInstalling collected packages: safetensors, xxhash, lightning-utilities, einops, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets, torchmetrics, pytorch_lightning\nSuccessfully installed datasets-2.14.5 dill-0.3.7 einops-0.7.0 huggingface-hub-0.16.4 lightning-utilities-0.9.0 multiprocess-0.70.15 pytorch_lightning-2.0.9.post0 safetensors-0.3.3 tokenizers-0.14.0 torchmetrics-1.2.0 transformers-4.34.0 xxhash-3.3.0\n\n\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport multiprocessing\n\nimport pytorch_lightning as pl\nfrom einops import rearrange # einstein operation\n\n\nfrom google.colab import drive\n\n# Mount Google Drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\nclass InferenceParams(nn.Module):\n    def __init__(self):\n\n        self.weight_decay = 0.01\n        self.adam_beta1 = 0.9\n        self.adam_beta2 = 0.98\n        self.adam_epsilon = 1e-7\n\n        self.layer_norm_epsilon = 1e-05\n\n        self.embd_pdrop = 0.0\n        self.resid_pdrop = 0.0\n        self.attention_pdrop = 0.0\n\n        self.activation_function = \"GELU\"\n\n        self.n_epochs = 10\n        self.initializer_range = 0.02\n        self.learning_rate = 5e-4 #3e-4\n        self.rotary_dim = 10 #10\n        self.n_layer = 4 #4\n        self.hidden_size = None\n        self.n_head =  8 #8\n        self.n_embd =  280  #280\n        self.vocab_size = 50257\n        self.max_sequence_len = 256 #512\n        self.max_batch_size = 32 #32\n\n\nconfig = InferenceParams()\n\n\nsample_train = 10 ** 5\nsample_val = int(.1 * sample_train)\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token\n\ntrain_subset = dataset['train'][:sample_train]['text']\nval_subset = dataset['validation'][:sample_val]['text']\n\ntokenized_trainset = tokenizer(\n    train_subset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=config.max_sequence_len  # Set the maximum sequence length\n)\n\ntokenized_valset = tokenizer(\n    val_subset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=config.max_sequence_len  # Set the maximum sequence length\n)\n\n\n\n\nRepo card metadata block was not found. Setting CardData to empty.\nWARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.data = tokenized_data\n\n    def __len__(self):\n        return len(self.data['input_ids'])\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.data['input_ids'][idx],\n            'attention_mask': self.data['attention_mask'][idx]\n        }\n\ntrain_data = CustomDataset(tokenized_trainset)\nval_data = CustomDataset(tokenized_valset)\n\ncpu_count = multiprocessing.cpu_count()\n\ntrain_loader = DataLoader(train_data, batch_size=config.max_batch_size, shuffle=True, num_workers=cpu_count)\nval_loader = DataLoader(val_data, batch_size=config.max_batch_size, num_workers=cpu_count)\n\n\nclass Embedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        self.drop = nn.Dropout(config.embd_pdrop)\n\n    def forward(self, input_ids):\n        input_shape = input_ids.shape[-1]\n        input_ids = input_ids.view(-1, input_shape)\n\n        hidden_states = self.wte(input_ids)\n        hidden_states = self.drop(hidden_states)\n\n        return hidden_states\n\n\nclass RotaryPositionEmbedding(nn.Module):\n    def __init__(self, config, base = 10000):\n        super().__init__()\n        self.dim = config.rotary_dim\n\n        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        self.cos_cache = None\n        self.sin_cache = None\n\n    def forward(self, qkv):\n        seqlen = qkv.shape[1]\n        # Update cos sin cache\n        t = torch.arange(seqlen, device = qkv.device)\n        freqs = torch.outer(t, self.inv_freq)\n\n        self.cos_cache = torch.cos(freqs)\n        self.sin_cache = torch.sin(freqs)\n\n        # Apply rotary qkv\n        rotary_dim = self.cos_cache.shape[1]\n        rotary_dim *= 2\n\n\n        q_rot = qkv[:, :, 0, :, :rotary_dim]\n        q_pass = qkv[:, :, 0, :, rotary_dim:]\n\n        k_rot = qkv[:, :, 1, :, :rotary_dim]\n        k_pass = qkv[:, :, 1, :, rotary_dim:]\n\n        # Splits the queries and keys in half\n        q1, q2 = q_rot.chunk(2, dim=-1)\n        k1, k2 = k_rot.chunk(2, dim=-1)\n        c = rearrange(self.cos_cache, \"t d -&gt; t 1 d\")\n        s = rearrange(self.sin_cache, \"t d -&gt; t 1 d\")\n\n        # Computes the new keys and queries\n        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n\n        return torch.cat(\n            [\n                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n                qkv[:, :, 2:3, :, :]\n            ],\n            dim=2\n        )\n\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        n_inner = 4 * config.n_embd if config.hidden_size is None \\\n                                            else config.hidden_size\n\n        self.fc1 = nn.Linear(config.n_embd, n_inner)\n        self.fc2 = nn.Linear(n_inner, config.n_embd)\n        self.act = getattr(torch.nn, config.activation_function)()\n\n    def forward(self, hidden_states):\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n\n        return hidden_states\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.drop = nn.Dropout(config.attention_pdrop)\n\n    def forward(self, qkv, attention_mask = None):\n        batch_size, seq_len = qkv.shape[0], qkv.shape[1]\n        q, k, v = qkv.unbind(2)\n\n        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n        scores = torch.einsum(\"bthd, bshd -&gt; bhts\", q, k * softmax_scale)\n\n        if attention_mask is not None:\n            padding_mask = torch.full((batch_size, seq_len), -10000.0, device=scores.device)\n            padding_mask.masked_fill_(attention_mask, 0.0)\n\n            scores = scores + rearrange(padding_mask, \"b s -&gt; b 1 1 s\")\n\n        casual_mask = torch.triu(torch.full((seq_len, seq_len), -10000, device=scores.device), 1)\n        scores += casual_mask\n\n        attention = torch.softmax(scores, dim=-1)\n        attention = self.drop(attention)\n\n        output = torch.einsum(\"bhts, bshd -&gt; bthd\", attention, v)\n\n        return output\n\n\nclass MHA(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.rotary_emb = RotaryPositionEmbedding(config)\n\n        self.head_dim = config.n_embd // config.n_head\n        opt_size = config.n_head * self.head_dim\n        hidden_size = config.n_embd\n\n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n\n        self.inner_attn = SelfAttention()\n\n    def forward(self, x, attention_mask = None):\n        qkv = self.Wqkv(x)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n\n        qkv = self.rotary_emb(qkv)\n\n        if attention_mask is not None:\n            attention_mask = attention_mask.bool().to(qkv.device)\n\n        output = self.inner_attn(qkv, attention_mask)\n\n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n        attn_out = self.out_proj(output)\n\n        return attn_out\n\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.ln = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n\n        self.attn = MHA(config)\n        self.ffwd = MLP(config)\n\n    def forward(self, hidden_states, attention_mask = None):\n        residual = hidden_states\n        hidden_states = self.ln(hidden_states)\n\n        attn_out = self.attn(hidden_states, attention_mask)\n        ffwd_out = self.ffwd(hidden_states)\n\n        attn_out = self.resid_dropout(attn_out)\n        ffwd_out = self.resid_dropout(ffwd_out)\n\n        output = attn_out + ffwd_out + residual\n        return output\n\n\nclass LMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.ln = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n        self.linear = nn.Linear(config.n_embd, config.vocab_size)\n\n    def forward(self, output):\n        output = self.ln(output)\n        logits = self.linear(output)\n\n        return logits\n\n\nclass SequentialForLM(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.initializer_range = config.initializer_range\n\n        modules = [Embedding(config)]\n        modules += [Block(config) for _ in range(config.n_layer)]\n        modules.append(LMHead(config))\n\n        self.layers = nn.Sequential(*modules)\n\n        self.apply(self._init_weights)\n\n    def forward(self, input_ids, attention_mask = None):\n        if attention_mask is not None and self.training:\n            print(\"`attention_mask` is not supported during training. Using it might lead to unexpected results.\")\n\n        if attention_mask is None:\n            logits = self.layers(input_ids)\n        else:\n            hidden_layer = self.layers[0](input_ids)\n            for module in self.layers[1:-1]:\n                hidden_layer = module(hidden_layer, attention_mask=attention_mask)\n            logits = self.layers[-1](hidden_layer)\n\n        return logits\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n\nclass LMLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.loss_fct = nn.CrossEntropyLoss()\n\n    def forward(self, logits, labels):\n\n        logits = logits[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n\n        loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        return loss\n\n\nclass ModelForVisualization(pl.LightningModule):\n    def __init__(self, config, is_load_state_dict, model_path):\n        super().__init__()\n\n        self.model_path = model_path\n\n        self.weight_decay = config.weight_decay\n        self.betas = (config.adam_beta1, config.adam_beta2)\n        self.epsilon = config.adam_epsilon\n\n        self.learning_rate = config.learning_rate\n\n        self.model = SequentialForLM(config)\n\n        if is_load_state_dict:\n            self.model.load_state_dict(torch.load(self.model_path,  map_location=torch.device('cpu')))\n\n        self.loss = LMLoss()\n\n\n    def forward(self, input_ids, attention_mask=None):\n        return self.model(input_ids, attention_mask)\n\n    def training_step(self, batch, batch_idx):\n        input_ids = batch['input_ids']\n\n        logits = self(input_ids)\n        loss = self.loss(logits, input_ids)\n\n        self.log(\"train loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n\n        if (batch_idx + 1) % 1000 == 0:\n            torch.save(self.model.state_dict(), self.model_path)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n\n        logits = self(input_ids, attention_mask)\n        loss = self.loss(logits, input_ids)\n\n        self.log(\"valid loss\", loss, prog_bar=True, on_step=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            lr=self.learning_rate,\n            weight_decay=self.weight_decay,\n            betas=self.betas,\n            eps=self.epsilon\n        )\n\n        return optimizer\n\n\nmodel_path = '/content/drive/MyDrive/microsof_phi15_model.pth'\nmodel = ModelForVisualization(config, True, model_path)\nlogger = pl.loggers.TensorBoardLogger('/content/drive/MyDrive/')\n\ntrainer = pl.Trainer(max_epochs=config.n_epochs, logger=logger, log_every_n_steps=1)\n\ntrainer.fit(model, train_loader, val_loader)\n\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name  | Type            | Params\n------------------------------------------\n0 | model | SequentialForLM | 32.0 M\n1 | loss  | LMLoss          | 0     \n------------------------------------------\n32.0 M    Trainable params\n0         Non-trainable params\n32.0 M    Total params\n127.881   Total estimated model params size (MB)\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")"
  },
  {
    "objectID": "code6.html",
    "href": "code6.html",
    "title": "Hyperparameters",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\n\nfrom einops import rearrange # einstein operation\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice\n\ndevice(type='cuda', index=0)"
  },
  {
    "objectID": "code6.html#download-dataset",
    "href": "code6.html#download-dataset",
    "title": "Hyperparameters",
    "section": "Download Dataset",
    "text": "Download Dataset\n\nclass InferenceParams(nn.Module):\n    def __init__(self):\n\n        self.weight_decay = 0.1\n        self.adam_beta1 = 0.9\n        self.adam_beta2 = 0.95\n        self.adam_epsilon = 1e-7\n\n        self.layer_norm_epsilon = 1e-05\n\n        self.embd_pdrop = 0.0\n        self.resid_pdrop = 0.0\n        self.attention_pdrop = 0.0\n\n        self.activation_function = \"GELU\"\n\n        self.n_epochs = 5\n        self.initializer_range = 0.02\n        self.learning_rate = 5e-4 #5e-4\n        self.rotary_dim = 16 #16\n        self.n_layer = 4 #4\n        self.hidden_size = None\n        self.n_head =  16 #16\n        self.n_embd =  290  #290\n        self.vocab_size = 50257\n        self.max_sequence_len = 512 #512\n        self.max_batch_size = 32 #32\n\nconfig = InferenceParams()\n\n\nsample = 10000\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token\n\ntrain_subset = dataset['train'][:sample]['text']\nval_subset = dataset['validation'][:sample]['text']\n\ntokenized_trainset = tokenizer(\n    train_subset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=config.max_sequence_len  # Set the maximum sequence length\n)\n\ntokenized_valset = tokenizer(\n    val_subset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=config.max_sequence_len  # Set the maximum sequence length\n)\n\n\n\n\nRepo card metadata block was not found. Setting CardData to empty.\nWARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport os\nimport multiprocessing\n\ncpu_count = multiprocessing.cpu_count()\nprint(f\"Number of CPU cores: {cpu_count}\")\n\nNumber of CPU cores: 2\n\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.data = tokenized_data\n\n    def __len__(self):\n        return len(self.data['input_ids'])\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.data['input_ids'][idx],\n            'attention_mask': self.data['attention_mask'][idx]\n        }\n\ntrain_data = CustomDataset(tokenized_trainset)\nval_data = CustomDataset(tokenized_valset)\n\ntrain_loader = DataLoader(train_data, batch_size=config.max_batch_size, shuffle=True, num_workers=cpu_count)\nval_loader = DataLoader(val_data, batch_size=config.max_batch_size, num_workers=cpu_count)\n\nbatch = next(iter(train_loader))\ninput_ids = batch['input_ids']\nattention_mask = batch['attention_mask']\nprint(input_ids.shape, attention_mask.shape)\n\ntorch.Size([32, 512]) torch.Size([32, 512])\n\n\n\nclass Embedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        self.drop = nn.Dropout(config.embd_pdrop)\n\n    def forward(self, input_ids):\n        input_shape = input_ids.shape[-1]\n        input_ids = input_ids.view(-1, input_shape)\n\n        hidden_states = self.wte(input_ids)\n        hidden_states = self.drop(hidden_states)\n\n        return hidden_states\n\n\nclass RotaryPositionEmbedding(nn.Module):\n    def __init__(self, config, base = 10000):\n        super().__init__()\n        self.dim = config.rotary_dim\n\n        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        self.cos_cache = None\n        self.sin_cache = None\n\n    def forward(self, qkv):\n        seqlen = qkv.shape[1]\n        # Update cos sin cache\n        t = torch.arange(seqlen, device = qkv.device)\n        freqs = torch.outer(t, self.inv_freq)\n\n        self.cos_cache = torch.cos(freqs)\n        self.sin_cache = torch.sin(freqs)\n\n        # Apply rotary qkv\n        rotary_dim = self.cos_cache.shape[1]\n        rotary_dim *= 2\n\n\n        q_rot = qkv[:, :, 0, :, :rotary_dim]\n        q_pass = qkv[:, :, 0, :, rotary_dim:]\n\n        k_rot = qkv[:, :, 1, :, :rotary_dim]\n        k_pass = qkv[:, :, 1, :, rotary_dim:]\n\n        # Splits the queries and keys in half\n        q1, q2 = q_rot.chunk(2, dim=-1)\n        k1, k2 = k_rot.chunk(2, dim=-1)\n        c = rearrange(self.cos_cache, \"t d -&gt; t 1 d\")\n        s = rearrange(self.sin_cache, \"t d -&gt; t 1 d\")\n\n        # Computes the new keys and queries\n        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n\n        return torch.cat(\n            [\n                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n                qkv[:, :, 2:3, :, :]\n            ],\n            dim=2\n        )\n\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        n_inner = 4 * config.n_embd if config.hidden_size is None \\\n                                            else config.hidden_size\n\n        self.fc1 = nn.Linear(config.n_embd, n_inner)\n        self.fc2 = nn.Linear(n_inner, config.n_embd)\n        self.act = getattr(torch.nn, config.activation_function)()\n\n    def forward(self, hidden_states):\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n\n        return hidden_states\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.drop = nn.Dropout(config.attention_pdrop)\n\n    def forward(self, qkv, attention_mask = None):\n        batch_size, seq_len = qkv.shape[0], qkv.shape[1]\n        q, k, v = qkv.unbind(2)\n\n        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n        scores = torch.einsum(\"bthd, bshd -&gt; bhts\", q, k * softmax_scale)\n\n        if attention_mask is not None:\n            padding_mask = torch.full((batch_size, seq_len), -10000.0, device=scores.device)\n            padding_mask.masked_fill_(attention_mask, 0.0)\n\n            scores = scores + rearrange(padding_mask, \"b s -&gt; b 1 1 s\")\n\n        casual_mask = torch.triu(torch.full((seq_len, seq_len), -10000, device=scores.device), 1)\n        scores += casual_mask\n\n        attention = torch.softmax(scores, dim=-1)\n        attention = self.drop(attention)\n\n        output = torch.einsum(\"bhts, bshd -&gt; bthd\", attention, v)\n\n        return output\n\n\nclass MHA(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.rotary_emb = RotaryPositionEmbedding(config)\n\n        self.head_dim = config.n_embd // config.n_head\n        opt_size = config.n_head * self.head_dim\n        hidden_size = config.n_embd\n\n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n\n        self.inner_attn = SelfAttention()\n\n    def forward(self, x, attention_mask = None):\n        qkv = self.Wqkv(x)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n\n        qkv = self.rotary_emb(qkv)\n\n        if attention_mask is not None:\n            attention_mask = attention_mask.bool().to(qkv.device)\n\n        output = self.inner_attn(qkv, attention_mask)\n\n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n        attn_out = self.out_proj(output)\n\n        return attn_out\n\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.ln = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n\n        self.attn = MHA(config)\n        self.ffwd = MLP(config)\n\n    def forward(self, hidden_states, attention_mask = None):\n        residual = hidden_states\n        hidden_states = self.ln(hidden_states)\n\n        attn_out = self.attn(hidden_states, attention_mask)\n        ffwd_out = self.ffwd(hidden_states)\n\n        attn_out = self.resid_dropout(attn_out)\n        ffwd_out = self.resid_dropout(ffwd_out)\n\n        output = attn_out + ffwd_out + residual\n        return output\n\n\nclass LMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.ln = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n        self.linear = nn.Linear(config.n_embd, config.vocab_size)\n\n    def forward(self, output):\n        output = self.ln(output)\n        logits = self.linear(output)\n\n        return logits\n\n\nclass SequentialForLM(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.initializer_range = config.initializer_range\n\n        modules = [Embedding(config)]\n        modules += [Block(config) for _ in range(config.n_layer)]\n        modules.append(LMHead(config))\n\n        self.layers = nn.Sequential(*modules)\n\n        self.apply(self._init_weights)\n\n    def forward(self, input_ids, attention_mask = None):\n        if attention_mask is not None and self.training:\n            print(\"`attention_mask` is not supported during training. Using it might lead to unexpected results.\")\n\n        if attention_mask is None:\n            logits = self.layers(input_ids)\n        else:\n            hidden_layer = self.layers[0](input_ids)\n            for module in self.layers[1:-1]:\n                hidden_layer = module(hidden_layer, attention_mask=attention_mask)\n            logits = self.layers[-1](hidden_layer)\n\n        return logits\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n\nclass LMLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.loss_fct = nn.CrossEntropyLoss()\n\n    def forward(self, logits, labels):\n\n        logits = logits[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n\n        loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        return loss\n\n\nclass ModelForVisualization(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n\n        self.weight_decay = config.weight_decay\n        self.betas = (config.adam_beta1, config.adam_beta2)\n        self.epsilon = config.adam_epsilon\n\n        self.learning_rate = config.learning_rate\n\n        self.model = SequentialForLM(config)\n        self.loss = LMLoss()\n\n    def forward(self, input_ids, attention_mask = None):\n        return self.model(input_ids, attention_mask)\n\n    def training_step(self, batch, batch_idx):\n        input_ids = batch['input_ids']\n        # attention_mask = batch['attention_mask']\n\n        logits = self(input_ids)\n        loss = self.loss(logits, input_ids)\n\n        self.log(\"train loss\", loss, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n\n        logits = self(input_ids, attention_mask)\n        loss = self.loss(logits, input_ids)\n\n        self.log(\"valid loss\", loss, prog_bar=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            lr=self.learning_rate,\n            weight_decay=self.weight_decay,\n            betas=self.betas,\n            eps=self.epsilon\n        )\n\n        return optimizer\n\n\nmodel = ModelForVisualization(config)\nfor layer in model.model.layers:\n    layer_class_name = type(layer).__name__\n    print(layer_class_name)\n\nEmbedding\nBlock\nBlock\nBlock\nBlock\nLMHead\n\n\n\nbatch = next(iter(train_loader))\n\ninput_data = batch['input_ids'][0]\nlayer_data = []\n\nfor module in model.model.layers:\n    output_data = module(input_data)\n    layer_name = type(module).__name__\n    print(f\"Output {layer_name}: {output_data.shape}\\n\")\n\n    input_data = output_data\n\n    layer_data.append((layer_name, output_data.detach().numpy().flatten()))\n\nOutput Embedding: torch.Size([1, 512, 290])\n\nOutput Block: torch.Size([1, 512, 290])\n\nOutput Block: torch.Size([1, 512, 290])\n\nOutput Block: torch.Size([1, 512, 290])\n\nOutput Block: torch.Size([1, 512, 290])\n\nOutput LMHead: torch.Size([1, 512, 50257])\n\n\n\n\nplt.figure(figsize=(8, 6))\nfor layer_name, layer_output in layer_data:\n    sns.kdeplot(layer_output, label=f\"Output {layer_name}\")\n\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.suptitle(\"Output Layer Density Plots\")\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nax = sns.boxplot(data=[data for _, data in layer_data], showfliers=False)\n\n# Set x-axis labels and title\nax.set_xlabel(\"Layer\")\nax.set_ylabel(\"Value\")\nax.set_title(\"Output Layer Box Plots\")\n\n# Set x-axis tick labels to display layer names\nax.set_xticklabels([layer_name for layer_name, _ in layer_data], rotation=45, ha='right')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nax = sns.violinplot(data=[data for _, data in layer_data], inner='box', cut=0)\n\n# Set x-axis labels and title\nax.set_xlabel(\"Layer\")\nax.set_ylabel(\"Value\")\nax.set_title(\"Output Layer Violin Plots\")\n\n# Set x-axis tick labels to display layer names\nax.set_xticklabels([layer_name for layer_name, _ in layer_data], rotation=45, ha='right')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nlogger = pl.loggers.TensorBoardLogger('logs/')\n\ntrainer = pl.Trainer(max_epochs=config.n_epochs, logger=logger)\n\ntrainer.fit(model, train_loader, val_loader)\n\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nWARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: logs/lightning_logs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name  | Type            | Params\n------------------------------------------\n0 | model | SequentialForLM | 33.2 M\n1 | loss  | LMLoss          | 0     \n------------------------------------------\n33.2 M    Trainable params\n0         Non-trainable params\n33.2 M    Total params\n132.961   Total estimated model params size (MB)\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# %reload_ext tensorboard\n# %tensorboard --logdir logs --port 6007\n\nReusing TensorBoard on port 6007 (pid 13793), started 0:00:08 ago. (Use '!kill 13793' to kill it.)\n\n\n\n\n\n\n\n\nTrain Loss\n\n\n\n\n\nValid Loss"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "microsoft_phi15_project",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "microsoft_phi15_project",
    "section": "Install",
    "text": "Install\npip install microsoft_phi15_project"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "microsoft_phi15_project",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "code4.html",
    "href": "code4.html",
    "title": "Init Weight",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport math\nimport numpy as np\n\nfrom einops import rearrange # einstein operation\n\nc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "code4.html#download-dataset",
    "href": "code4.html#download-dataset",
    "title": "Init Weight",
    "section": "Download Dataset",
    "text": "Download Dataset\n\nclass InferenceParams(nn.Module):\n    def __init__(self):\n\n        self.max_sequence_len = 30\n        self.rotary_dim = 3\n        self.n_layer = 4\n        self.batch_size = 32\n        self.n_embd = 20\n        self.n_head = 4\n        self.vocab_size = 50257\n        \nconfig = InferenceParams()\n\n\nsample = 50\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token  \n\nsubset_dataset = dataset['train'][:sample]['text']\n\ntokenized_dataset = tokenizer(\n    subset_dataset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=config.max_sequence_len  # Set the maximum sequence length\n)\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\n\ndef get_batch(data, batch_size):\n    idx = torch.randint(0, len(data), size=(batch_size,))\n    \n    return data[idx]\n\ndata = tokenized_dataset['input_ids']\ninput_ids = get_batch(data, config.batch_size)\ninput_ids.shape\n\ntorch.Size([32, 30])"
  },
  {
    "objectID": "code4.html#embedding",
    "href": "code4.html#embedding",
    "title": "Init Weight",
    "section": "Embedding",
    "text": "Embedding\n\nclass Embedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        \n    def forward(self, input_ids):\n        input_shape = input_ids.shape[-1]\n        input_ids = input_ids.view(-1, input_shape)\n        \n        hidden_states = self.wte(input_ids)\n        \n        return hidden_states\n\nm = Embedding(config)\nhidden_states = m(input_ids)\nhidden_states.shape\n\ntorch.Size([32, 30, 20])\n\n\n\nclass RotaryPositionEmbedding(nn.Module):\n    def __init__(self, config, base = 10000):\n        super().__init__()\n        self.dim = config.rotary_dim \n        \n        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n        \n        self.cos_cache = None\n        self.sin_cache = None\n        \n    def forward(self, qkv):\n        seqlen = qkv.shape[1]\n        \n        # Update cos sin cache\n        t = torch.arange(seqlen)\n        freqs = torch.outer(t, self.inv_freq)\n        \n        self.cos_cache = torch.cos(freqs)\n        self.sin_cache = torch.sin(freqs)\n        \n        # Apply rotary qkv\n        rotary_dim = self.cos_cache.shape[1]\n        rotary_dim *= 2\n        \n        q_rot = qkv[:, :, 0, :, :rotary_dim]\n        q_pass = qkv[:, :, 0, :, rotary_dim:]\n        \n        k_rot = qkv[:, :, 1, :, :rotary_dim]\n        k_pass = qkv[:, :, 1, :, rotary_dim:]\n        \n        # Splits the queries and keys in half\n        q1, q2 = q_rot.chunk(2, dim=-1)\n        k1, k2 = k_rot.chunk(2, dim=-1)\n        c, s = rearrange(self.cos_cache, \"t d -&gt; t 1 d\"), rearrange(self.sin_cache, \"t d -&gt; t 1 d\")\n        \n        # Computes the new keys and queries\n        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n        \n        return torch.cat(\n            [\n                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n                qkv[:, :, 2:3, :, :]\n            ],\n            dim=2\n        )"
  },
  {
    "objectID": "code4.html#mlp",
    "href": "code4.html#mlp",
    "title": "Init Weight",
    "section": "MLP",
    "text": "MLP\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        n_inner = 4 * config.n_embd\n        \n        self.fc1 = nn.Linear(config.n_embd, n_inner)\n        self.fc2 = nn.Linear(n_inner, config.n_embd)\n        self.act = nn.ReLU()\n        \n    def forward(self, hidden_states):\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        \n        return hidden_states\n    \nm = MLP(config)\nffwd_out = m(hidden_states)\nffwd_out.shape\n\ntorch.Size([32, 30, 20])"
  },
  {
    "objectID": "code4.html#attention",
    "href": "code4.html#attention",
    "title": "Init Weight",
    "section": "Attention",
    "text": "Attention\n\nclass SelfAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        pass\n            \n    def forward(self, qkv):\n        seq_len = qkv.shape[1]\n        q, k, v = qkv.unbind(2)\n        \n        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n        scores = torch.einsum(\"bthd, bshd -&gt; bhts\", q, k * softmax_scale)\n\n        casual_mask = torch.triu(torch.full((seq_len, seq_len), -10000), 1)\n        scores += casual_mask\n        \n        attention = torch.softmax(scores, dim=-1)\n        \n        output = torch.einsum(\"bhts, bshd -&gt; bthd\", attention, v)\n        \n        return output\n\n\nclass MHA(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.rotary_emb = RotaryPositionEmbedding(config)\n        \n        self.head_dim = config.n_embd // config.n_head\n        opt_size = config.n_head * self.head_dim\n        hidden_size = config.n_embd\n        \n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n        \n        self.inner_attn = SelfAttention()\n        \n    def forward(self, x):\n        qkv = self.Wqkv(x)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n        \n        qkv = self.rotary_emb(qkv)\n        \n        output = self.inner_attn(qkv)\n        \n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n        attn_out = self.out_proj(output)\n        \n        return attn_out\n    \nm = MHA(config)\nattn_out = m(hidden_states)\nattn_out.shape\n\ntorch.Size([32, 30, 20])"
  },
  {
    "objectID": "code4.html#block",
    "href": "code4.html#block",
    "title": "Init Weight",
    "section": "Block",
    "text": "Block\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        self.ln = nn.LayerNorm(config.n_embd)\n        \n        self.attn = MHA(config)\n        self.ffwd = MLP(config)\n        \n    def forward(self, hidden_states):\n        residual = hidden_states\n        hidden_states = self.ln(hidden_states)\n        \n        attn_out = self.attn(hidden_states)\n        ffwd_out = self.ffwd(hidden_states)\n        \n        output = attn_out + ffwd_out + residual\n        return output\n\nm = Block(config)\noutput = m(hidden_states)\noutput.shape\n\ntorch.Size([32, 30, 20])"
  },
  {
    "objectID": "code4.html#sequential",
    "href": "code4.html#sequential",
    "title": "Init Weight",
    "section": "Sequential",
    "text": "Sequential\n\nclass LMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        self.ln = nn.LayerNorm(config.n_embd)\n        self.linear = nn.Linear(config.n_embd, config.vocab_size)\n        \n    def forward(self, output):\n        output = self.ln(output)\n        logits = self.linear(output)\n        \n        return logits\n    \nm = LMHead(config)\nlogits = m(output)\nlogits.shape\n\ntorch.Size([32, 30, 50257])\n\n\n\nclass SequentialForLM_Not(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        modules = [Embedding(config)]\n        modules += [Block(config) for _ in range(config.n_layer)]\n        modules.append(LMHead(config))\n        \n        self.layers = nn.Sequential(*modules)\n        \n    def forward(self, input_ids):\n        return self.layers(input_ids)\n    \nm = SequentialForLM_Not(config)\nlogits = m(input_ids)\nlogits.shape\n\ntorch.Size([32, 30, 50257])"
  },
  {
    "objectID": "code4.html#loss",
    "href": "code4.html#loss",
    "title": "Init Weight",
    "section": "Loss",
    "text": "Loss\n\nclass LMLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.loss_fct = nn.CrossEntropyLoss()\n        \n    def forward(self, logits, labels):\n        \n        logits = logits[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()          \n                             \n        loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        return loss\n\nlm_loss = LMLoss()\nloss = lm_loss(logits, input_ids)\nloss\n\ntensor(10.9480, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "code4.html#experiment-init-weight",
    "href": "code4.html#experiment-init-weight",
    "title": "Init Weight",
    "section": "Experiment init weight",
    "text": "Experiment init weight\n\nfor name, param in m.named_parameters():\n    if 'weight' in name:\n        print(name)\n\nlayers.0.wte.weight\nlayers.1.ln.weight\nlayers.1.attn.Wqkv.weight\nlayers.1.attn.out_proj.weight\nlayers.1.ffwd.fc1.weight\nlayers.1.ffwd.fc2.weight\nlayers.2.ln.weight\nlayers.2.attn.Wqkv.weight\nlayers.2.attn.out_proj.weight\nlayers.2.ffwd.fc1.weight\nlayers.2.ffwd.fc2.weight\nlayers.3.ln.weight\nlayers.3.attn.Wqkv.weight\nlayers.3.attn.out_proj.weight\nlayers.3.ffwd.fc1.weight\nlayers.3.ffwd.fc2.weight\nlayers.4.ln.weight\nlayers.4.attn.Wqkv.weight\nlayers.4.attn.out_proj.weight\nlayers.4.ffwd.fc1.weight\nlayers.4.ffwd.fc2.weight\nlayers.5.ln.weight\nlayers.5.linear.weight\n\n\n\nmodel_not = SequentialForLM_Not(config)\n\nweight_names = {\n    \"Embedding\": 'layers.0.wte.weight',\n    \"Linear\": 'layers.1.attn.Wqkv.weight',\n    \"Layer Norm\": 'layers.1.ln.weight'\n}\n\nweight_dict_not = {}\nfor name, param in model_not.named_parameters():\n    param = param.view(-1).detach()\n    if name in weight_names.values():\n        param_name = [key for key, value in weight_names.items() if value == name][0]\n        weight_dict_not[param_name] = param\n\n\nimport matplotlib.pyplot as plt\n\ndef show_graph_init_weight(weight_dict, string):\n    fig, axs = plt.subplots(1, len(weight_dict), figsize=(14, 6))\n\n    for i, (param_name, param_value) in enumerate(weight_dict.items()):\n        ax = axs[i]\n        ax.hist(param_value, bins='fd')\n        \n        ax.set_title(f\"{param_name}\\n \\\n            Mean: {param_value.mean():.5f}, Std: {param_value.std():.5f}\", fontsize=14)\n        \n        ax.set_xlabel(\"Value\", fontsize=10)\n        ax.set_ylabel(\"Frequency\", fontsize=10)\n\n    fig.suptitle(f\"{string} Apply Initialize Weight\", fontsize=16)\n    plt.tight_layout()\n    plt.show()\n    \nshow_graph_init_weight(weight_dict_not, \"Not\")\n\n\n\n\n\nclass SequentialForLM_Have(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        modules = [Embedding(config)]\n        modules += [Block(config) for _ in range(config.n_layer)]\n        modules.append(LMHead(config))\n        \n        self.layers = nn.Sequential(*modules)\n        \n        self.apply(self._init_weights)\n        \n        \n    def forward(self, input_ids):\n        return self.layers(input_ids)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=.02)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=.02)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n\nmodel_have = SequentialForLM_Have(config)\n\nweight_dict_have = {}\nfor name, param in model_have.named_parameters():\n    param = param.view(-1).detach()\n    if name in weight_names.values():\n        param_name = [key for key, value in weight_names.items() if value == name][0]\n        weight_dict_have[param_name] = param\n        \nshow_graph_init_weight(weight_dict_have, \"Have\")\n\n\n\n\n\ndef get_layers_output(model, input_data):\n    layers_output = []\n    \n    hidden_layers = input_data[0]\n    for module in model.layers:\n        hidden_layers = module(hidden_layers)\n        layers_output.append(hidden_layers.detach().numpy().flatten())\n\n    return layers_output\n\nlayers_output_not = get_layers_output(model_not, input_ids)\nlayers_output_have = get_layers_output(model_have, input_ids)\n\n\nimport seaborn as sns\n\ndef show_density_plots(layers_output, ax, title):\n    for i, layer_output in enumerate(layers_output):\n        sns.kdeplot(layer_output, ax=ax, label=f\"Layer {i+1}\")\n        \n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Density\")\n    ax.legend()\n    ax.set_title(title)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nshow_density_plots(layers_output_not, axes[0], \"Density Plots (Not)\")\nshow_density_plots(layers_output_have, axes[1], \"Density Plots (Have)\")\n\nplt.show()\n\n\n\n\n\ndef show_box_plots(layers_output, ax, title):\n    sns.boxplot(data=layers_output, ax=ax, showfliers=False)\n        \n    ax.set_xlabel(\"Layer\")\n    ax.set_ylabel(\"Value\")\n    ax.set_title(title)\n    \nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nshow_box_plots(layers_output_not, axes[0], \"Box Plots (Not)\")\nshow_box_plots(layers_output_have, axes[1], \"Box Plots (Have)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\ndef show_violin_plots(layers_output, ax, title):\n    sns.violinplot(data=layers_output, ax=ax, inner='box', cut=0)\n    \n    ax.set_xlabel(\"Layer\")\n    ax.set_ylabel(\"Value\")\n    ax.set_title(title)        \n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nshow_violin_plots(layers_output_not, axes[0], \"Violin Plots (Not)\")\nshow_violin_plots(layers_output_have, axes[1], \"Violin Plots (Have)\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "code1.html",
    "href": "code1.html",
    "title": "Overview",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer"
  },
  {
    "objectID": "code1.html#download-dataset",
    "href": "code1.html#download-dataset",
    "title": "Overview",
    "section": "Download Dataset",
    "text": "Download Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ndataset\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 2119719\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 21990\n    })\n})\n\n\n\nsample = 20\n\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token  \n\nsubset_dataset = dataset['train'][:sample]['text']\n\n# Tokenize the text data in the new subset dataset with padding and truncation\ntokenized_dataset = tokenizer(\n    subset_dataset,\n    return_tensors='pt',\n    padding=True,  # Enable padding\n    truncation=True  # Enable truncation\n)\n\n\nfor example in tokenized_dataset['input_ids'][:3]:\n    # Decode the list of token IDs\n    decoded_text = tokenizer.decode(example, skip_special_tokens=True)\n    print(decoded_text)\n\nOne day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n\nTogether, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\nOnce upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\n\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\n\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.\nOne day, a little fish named Fin was swimming near the shore. He saw a big crab and wanted to be friends. \"Hi, I am Fin. Do you want to play?\" asked the little fish. The crab looked at Fin and said, \"No, I don't want to play. I am cold and I don't feel fine.\"\n\nFin felt sad but wanted to help the crab feel better. He swam away and thought of a plan. He remembered that the sun could make things warm. So, Fin swam to the top of the water and called to the sun, \"Please, sun, help my new friend feel fine and not freeze!\"\n\nThe sun heard Fin's call and shone its warm light on the shore. The crab started to feel better and not so cold. He saw Fin and said, \"Thank you, little fish, for making me feel fine. I don't feel like I will freeze now. Let's play together!\" And so, Fin and the crab played and became good friends.\n\n\n\ndata = tokenized_dataset['input_ids']\ndata.shape\n\ntorch.Size([20, 219])\n\n\n\nx = data[:, :-1].contiguous()\ny = data[:, 1:].contiguous()\nx.shape, y.shape\n\n(torch.Size([20, 218]), torch.Size([20, 218]))\n\n\n\nfor i in range(5):\n    print(f\"Input: {x[0, :i+1]} --&gt; Labels: {y[0,i]}\")\n\nInput: tensor([3198]) --&gt; Labels: 1110\nInput: tensor([3198, 1110]) --&gt; Labels: 11\nInput: tensor([3198, 1110,   11]) --&gt; Labels: 257\nInput: tensor([3198, 1110,   11,  257]) --&gt; Labels: 1310\nInput: tensor([3198, 1110,   11,  257, 1310]) --&gt; Labels: 2576\n\n\n\nvocab_size = tokenizer.vocab_size\nsequence_len = x.size(1)\n\nprint(vocab_size, sequence_len)\n\n50257 218"
  },
  {
    "objectID": "code1.html#embedding",
    "href": "code1.html#embedding",
    "title": "Overview",
    "section": "Embedding",
    "text": "Embedding\n\n# Token Embedding\nn_embd = 36\nwte = nn.Embedding(vocab_size, n_embd) # word to embedding\n\ntoken_embd = wte(x)\ntoken_embd.shape\n\ntorch.Size([20, 218, 36])\n\n\n\n# Position Embedding\nposition = nn.Embedding(sequence_len, n_embd)\n\nposition_embd = position(torch.arange(sequence_len))\nposition_embd.shape\n\ntorch.Size([218, 36])\n\n\n\nx_embd = token_embd + position_embd\nx_embd.shape\n\ntorch.Size([20, 218, 36])"
  },
  {
    "objectID": "code1.html#mlp",
    "href": "code1.html#mlp",
    "title": "Overview",
    "section": "MLP",
    "text": "MLP\n\nln_hidden_states = nn.LayerNorm(n_embd)\nx_embd_ln = ln_hidden_states(x_embd)\n\n\nfc1 = nn.Linear(n_embd, 4 * n_embd)\nfc2 = nn.Linear(4 * n_embd, n_embd)\n\nact = nn.ReLU()\n\n\n# Feed forward output\nffwd_out = fc1(x_embd_ln)\nffwd_out = act(ffwd_out)\nffwd_out = fc2(ffwd_out)\n\nffwd_out.shape\n\ntorch.Size([20, 218, 36])"
  },
  {
    "objectID": "code1.html#attension",
    "href": "code1.html#attension",
    "title": "Overview",
    "section": "Attension",
    "text": "Attension\n\nn_head = 4\n\nhead_size = n_embd // n_head\nopt_size = n_head * head_size # output size\nhead_size, opt_size\n\n(9, 36)\n\n\n\nWqkv = nn.Linear(n_embd, 3 * opt_size)\nqkv = Wqkv(x_embd_ln)\nqkv.shape\n\ntorch.Size([20, 218, 108])\n\n\n\nfrom einops import rearrange\nqkv = rearrange(qkv, \"... (three h d) -&gt; ... three h d\", three=3, h = n_head)\nqkv.shape\n\ntorch.Size([20, 218, 3, 4, 9])\n\n\n\nq, k, v = qkv.unbind(dim=2)\nq.shape, k.shape, v.shape\n\n(torch.Size([20, 218, 4, 9]),\n torch.Size([20, 218, 4, 9]),\n torch.Size([20, 218, 4, 9]))\n\n\n\nimport math\nsoftmax_scale = 1.0 / math.sqrt(q.shape[-1])\n\nscores = torch.einsum(\"bthd,bshd-&gt;bhts\", q, k * softmax_scale)\nscores.shape\n\ntorch.Size([20, 4, 218, 218])\n\n\n\nmask = torch.triu(torch.full((sequence_len, sequence_len), -10000.0), 1)\nscores = scores + mask\nscores.shape\n\ntorch.Size([20, 4, 218, 218])\n\n\n\nattention = torch.softmax(scores, dim=-1)\n\nattn_out = torch.einsum(\"bhts,bshd-&gt;bthd\", attention, v)\n\nattn_out = rearrange(attn_out, \"... h d -&gt; ... (h d)\")\nattn_out.shape\n\ntorch.Size([20, 218, 36])\n\n\n\nout_proj = nn.Linear(opt_size, n_embd)\nattn_out = out_proj(attn_out)\nattn_out.shape\n\ntorch.Size([20, 218, 36])"
  },
  {
    "objectID": "code1.html#transformer-block",
    "href": "code1.html#transformer-block",
    "title": "Overview",
    "section": "Transformer Block",
    "text": "Transformer Block\n\nresidual = x_embd\n\noutput = ffwd_out + attn_out + residual\noutput.shape\n\ntorch.Size([20, 218, 36])\n\n\n\nlinear = nn.Linear(n_embd, vocab_size)\nln_output = nn.LayerNorm(n_embd)\n\noutput = ln_output(output)\n\nlogits = linear(output)\nlogits.shape\n\ntorch.Size([20, 218, 50257])"
  },
  {
    "objectID": "code1.html#loss",
    "href": "code1.html#loss",
    "title": "Overview",
    "section": "Loss",
    "text": "Loss\n\nloss_fct = nn.CrossEntropyLoss()\n\nlogits  = logits.view(-1, logits.shape[-1])\nlabels = y.view(-1)\n\nloss = loss_fct(logits, labels)\nloss\n\ntensor(11.0906, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "code2.html",
    "href": "code2.html",
    "title": "Basic Class",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport math\n\nfrom einops import rearrange # einstein operation\n\nc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "code2.html#download-dataset",
    "href": "code2.html#download-dataset",
    "title": "Basic Class",
    "section": "Download Dataset",
    "text": "Download Dataset\n\nsample = 100\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token  # You can choose any appropriate token for padding\n\nsubset_dataset = dataset['train'][:sample]['text']\ntokenized_dataset = tokenizer(\n    subset_dataset,\n    return_tensors='pt',\n    padding=True,  # Enable padding\n    truncation=True  # Enable truncation\n)\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\n\ndata = tokenized_dataset['input_ids']\ndata.shape\n\ntorch.Size([100, 298])\n\n\n\n# InferenceParams\n\nbatch_size = 16\n\nn_head = 4\nn_embd = 36\nsequence_len = data.size(1) - 1\nvocab_size = tokenizer.vocab_size\n\n\ndef get_batch(data, batch_size):\n    idx = torch.randint(0, len(data), size=(batch_size,))\n    batch = data[idx]\n\n    xb = batch[:, :-1].contiguous()\n    yb = batch[:, 1:].contiguous()\n    \n    return xb, yb\n\nxb, yb = get_batch(data, batch_size)\nxb.shape, yb.shape\n\n(torch.Size([16, 297]), torch.Size([16, 297]))"
  },
  {
    "objectID": "code2.html#embedding",
    "href": "code2.html#embedding",
    "title": "Basic Class",
    "section": "Embedding",
    "text": "Embedding\n\nclass Embedding(nn.Module):\n    def __init__(self, vocab_size, n_embd, sequence_len):\n        super().__init__()\n        self.sequence_len = sequence_len\n        \n        self.wte = nn.Embedding(vocab_size, n_embd)\n        self.position = nn.Embedding(sequence_len, n_embd)\n        \n    def forward(self, input_ids):\n        token_embd = self.wte(input_ids)\n        position_embd = self.position(torch.arange(self.sequence_len))\n        \n        hidden_states = token_embd + position_embd        \n        \n        return hidden_states\n\n\nm = Embedding(vocab_size, n_embd, sequence_len)\nhidden_states = m(xb)\nhidden_states.shape\n\ntorch.Size([16, 297, 36])"
  },
  {
    "objectID": "code2.html#mlp",
    "href": "code2.html#mlp",
    "title": "Basic Class",
    "section": "MLP",
    "text": "MLP\n\nclass MLP(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        n_inner = 4 * n_embd\n        \n        self.fc1 = nn.Linear(n_embd, n_inner)\n        self.fc2 = nn.Linear(n_inner, n_embd)\n        self.act = nn.ReLU()\n        \n    def forward(self, hidden_states):\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        \n        return hidden_states\n\n\nm = MLP(n_embd)\nffwd_out = m(hidden_states)\nffwd_out.shape\n\ntorch.Size([16, 297, 36])"
  },
  {
    "objectID": "code2.html#attention",
    "href": "code2.html#attention",
    "title": "Basic Class",
    "section": "Attention",
    "text": "Attention\n\nclass SelfAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        pass\n        \n    def forward(self, qkv):\n        seq_len = qkv.shape[1]\n        q, k, v = qkv.unbind(2)\n        \n        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n        scores = torch.einsum(\"bthd, bshd -&gt; bhts\", q, k * softmax_scale)\n        \n        mask = torch.triu(torch.full((seq_len, seq_len), -10000), 1)\n        scores += mask\n        \n        attention = torch.softmax(scores, dim=-1)\n        \n        output = torch.einsum(\"bhts, bshd -&gt; bthd\", attention, v)\n        \n        return output\n\n\nclass MHA(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        self.head_dim = n_embd // n_head\n        opt_size = n_head * self.head_dim\n        hidden_size = n_embd\n        \n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n        \n        self.inner_attn = SelfAttention()\n        \n    def forward(self, x):\n        qkv = self.Wqkv(x)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n        \n        output = self.inner_attn(qkv)\n        \n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n        attn_out = self.out_proj(output)\n        \n        return attn_out\n\n\nm = MHA(n_embd, n_head)\nattn_out = m(hidden_states)\nattn_out.shape\n\ntorch.Size([16, 297, 36])"
  },
  {
    "objectID": "code2.html#block",
    "href": "code2.html#block",
    "title": "Basic Class",
    "section": "Block",
    "text": "Block\n\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        \n        self.ln = nn.LayerNorm(n_embd)\n        \n        self.ffwd = MLP(n_embd)\n        self.attn = MHA(n_embd, n_head)\n        \n    def forward(self, hidden_states):\n        residual = hidden_states\n        hidden_states = self.ln(hidden_states)\n        \n        attn_out = self.attn(hidden_states)\n        \n        ffwd_out = self.ffwd(hidden_states)\n        \n        output = attn_out + ffwd_out + residual\n        return output\n\n\nm = Block(n_embd, n_head)\noutput = m(hidden_states)\noutput.shape\n\ntorch.Size([16, 297, 36])\n\n\n\nclass LMHead(nn.Module):\n    def __init__(self, vocab_size, n_embd):\n        super().__init__()\n        \n        self.ln = nn.LayerNorm(n_embd)\n        self.linear = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, output):\n        output = self.ln(output)\n        logits = self.linear(output)\n        \n        return logits\n\n\nm = LMHead(vocab_size, n_embd)\nlogits = m(output)\nlogits.shape\n\ntorch.Size([16, 297, 50257])"
  },
  {
    "objectID": "code2.html#loss",
    "href": "code2.html#loss",
    "title": "Basic Class",
    "section": "Loss",
    "text": "Loss\n\nclass LMLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.loss_fct = nn.CrossEntropyLoss()\n        \n    def forward(self, logits, labels):              \n                             \n        loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        return loss\n\n\nlm_loss = LMLoss()\nloss = lm_loss(logits, yb)\nloss\n\ntensor(11.1934, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "code3.html",
    "href": "code3.html",
    "title": "Rotary Embedding",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport math\n\nfrom einops import rearrange # einstein operation"
  },
  {
    "objectID": "code3.html#download-dataset",
    "href": "code3.html#download-dataset",
    "title": "Rotary Embedding",
    "section": "Download Dataset",
    "text": "Download Dataset\n\nsample = 100\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token  # You can choose any appropriate token for padding\n\nsubset_dataset = dataset['train'][:sample]['text']\ntokenized_dataset = tokenizer(\n    subset_dataset,\n    return_tensors='pt',\n    padding=True,  # Enable padding\n    truncation=True  # Enable truncation\n)\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\n\ndata = tokenized_dataset['input_ids']\nprint(tokenizer.vocab_size)\n\n50257\n\n\n\n# InferenceParams\nclass InferenceParams(nn.Module):\n    def __init__(self, sequence_len):\n        \n        self.rotary_dim = 3\n        self.n_layer = 2\n        \n        self.sequence_len = sequence_len\n        self.batch_size = 16\n        self.n_embd = 20\n        self.n_head = 4\n        self.vocab_size = 50257\n\n\nsequence_len = data.size(1) - 1\nconfig = InferenceParams(sequence_len)\n\n\ndef get_batch(data, batch_size):\n    idx = torch.randint(0, len(data), size=(batch_size,))\n    batch = data[idx]\n\n    xb = batch[:, :-1].contiguous()\n    yb = batch[:, 1:].contiguous()\n    \n    return xb, yb\n\nxb, yb = get_batch(data, config.batch_size)\nxb.shape, yb.shape\n\n(torch.Size([16, 297]), torch.Size([16, 297]))"
  },
  {
    "objectID": "code3.html#embedding",
    "href": "code3.html#embedding",
    "title": "Rotary Embedding",
    "section": "Embedding",
    "text": "Embedding\n\nclass Embedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        \n    def forward(self, input_ids):\n        hidden_states = self.wte(input_ids)\n        \n        return hidden_states\n\n\nm = Embedding(config)\nhidden_states = m(xb)\nhidden_states.shape\n\ntorch.Size([16, 297, 20])\n\n\n\nclass RotaryPositionEmbedding(nn.Module):\n    def __init__(self, config, base = 10000):\n        super().__init__()\n        self.rotary_dim  = config.rotary_dim \n        \n        inv_freq = 1.0 / (base ** (torch.arange(0, self.rotary_dim, 2) / self.rotary_dim ))\n        self.register_buffer(\"inv_freq\", inv_freq)\n        \n        self.cos_cache = None\n        self.sin_cache = None\n        \n    def forward(self, qkv):\n        seqlen = qkv.shape[1]\n        \n        # Update cos sin cache\n        t = torch.arange(seqlen)\n        freqs = torch.outer(t, self.inv_freq)\n        \n        self.cos_cache = torch.cos(freqs)\n        self.sin_cache = torch.sin(freqs)\n        \n        # Apply rotary qkv\n        rotary_dim = self.cos_cache.shape[1]\n        rotary_dim *= 2\n        \n        q_rot = qkv[:, :, 0, :, :rotary_dim]\n        q_pass = qkv[:, :, 0, :, rotary_dim:]\n        \n        k_rot = qkv[:, :, 1, :, :rotary_dim]\n        k_pass = qkv[:, :, 1, :, rotary_dim:]\n        \n        # Splits the queries and keys in half\n        q1, q2 = q_rot.chunk(2, dim=-1)\n        k1, k2 = k_rot.chunk(2, dim=-1)\n        c, s = rearrange(self.cos_cache, \"t d -&gt; t 1 d\"), rearrange(self.sin_cache, \"t d -&gt; t 1 d\")\n        \n        # Computes the new keys and queries\n        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n        \n        return torch.cat(\n            [\n                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n                qkv[:, :, 2:3, :, :]\n            ],\n            dim=2\n        )"
  },
  {
    "objectID": "code3.html#mlp",
    "href": "code3.html#mlp",
    "title": "Rotary Embedding",
    "section": "MLP",
    "text": "MLP\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        n_inner = 4 * config.n_embd\n        \n        self.fc1 = nn.Linear(config.n_embd, n_inner)\n        self.fc2 = nn.Linear(n_inner, config.n_embd)\n        self.act = nn.GELU()\n        \n    def forward(self, hidden_states):\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        \n        return hidden_states\n\n\nm = MLP(config)\nffwd_out = m(hidden_states)\nffwd_out.shape\n\ntorch.Size([16, 297, 20])"
  },
  {
    "objectID": "code3.html#attention",
    "href": "code3.html#attention",
    "title": "Rotary Embedding",
    "section": "Attention",
    "text": "Attention\n\nclass SelfAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        pass\n        \n    def forward(self, qkv):\n        seq_len = qkv.shape[1]\n        q, k, v = qkv.unbind(2)\n        \n        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n        scores = torch.einsum(\"bthd, bshd -&gt; bhts\", q, k * softmax_scale)\n        \n        mask = torch.triu(torch.full((seq_len, seq_len), -10000), 1)\n        scores += mask\n        \n        attention = torch.softmax(scores, dim=-1)\n        \n        output = torch.einsum(\"bhts, bshd -&gt; bthd\", attention, v)\n        \n        return output\n\n\nclass MHA(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        self.rotary_emb = RotaryPositionEmbedding(config)\n        \n        self.head_dim = config.n_embd // config.n_head\n        opt_size = config.n_head * self.head_dim\n        hidden_size = config.n_embd\n        \n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n        \n        self.inner_attn = SelfAttention()\n        \n    def forward(self, x):\n        qkv = self.Wqkv(x)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n        \n        qkv = self.rotary_emb(qkv)\n        \n        output = self.inner_attn(qkv)\n        \n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n        attn_out = self.out_proj(output)\n        \n        return attn_out\n\n\nm = MHA(config)\nattn_out = m(hidden_states)\nattn_out.shape\n\ntorch.Size([16, 297, 20])"
  },
  {
    "objectID": "code3.html#block",
    "href": "code3.html#block",
    "title": "Rotary Embedding",
    "section": "Block",
    "text": "Block\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        self.ln = nn.LayerNorm(config.n_embd)\n        \n        self.attn = MHA(config)\n        self.ffwd = MLP(config)\n        \n    def forward(self, hidden_states):\n        residual = hidden_states\n        hidden_states = self.ln(hidden_states)\n        \n        attn_out = self.attn(hidden_states)\n        \n        ffwd_out = self.ffwd(hidden_states)\n        \n        output = attn_out + ffwd_out + residual\n        return output\n\n\nm = Block(config)\noutput = m(hidden_states)\noutput.shape\n\ntorch.Size([16, 297, 20])\n\n\n\nclass LMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        self.ln = nn.LayerNorm(config.n_embd)\n        self.linear = nn.Linear(config.n_embd, config.vocab_size)\n        \n    def forward(self, output):\n        output = self.ln(output)\n        logits = self.linear(output)\n        \n        return logits\n\n\nm = LMHead(config)\nlogits = m(output)\nlogits.shape\n\ntorch.Size([16, 297, 50257])"
  },
  {
    "objectID": "code3.html#sequential",
    "href": "code3.html#sequential",
    "title": "Rotary Embedding",
    "section": "Sequential",
    "text": "Sequential\n\nclass SequentialForLM(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        n_layers = 2\n        \n        modules = [Embedding(config)]\n        modules += [Block(config) for _ in range(n_layers)]\n        modules.append(LMHead(config))\n        \n        self.layers = nn.Sequential(*modules)\n        \n    def forward(self, input_ids):\n        logits = self.layers(input_ids)\n        return logits\n\n\nm = SequentialForLM(config)\nlogits = m(xb)\nlogits.shape\n\ntorch.Size([16, 297, 50257])"
  },
  {
    "objectID": "code3.html#loss",
    "href": "code3.html#loss",
    "title": "Rotary Embedding",
    "section": "Loss",
    "text": "Loss\n\nclass LMLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.loss_fct = nn.CrossEntropyLoss()\n        \n    def forward(self, logits, labels):\n        logits = logits.view(-1, logits.shape[-1])\n        labels = labels.view(-1)                    \n                             \n        loss = self.loss_fct(logits, labels)\n\n        return loss\n\n\nlm_loss = LMLoss()\nloss = lm_loss(logits, yb)\nloss\n\ntensor(11.1403, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\nconfig = InferenceParams(sequence_len)\nxb, yb = get_batch(data, config.batch_size)\n\nm = SequentialForLM(config)\nlogits = m(xb)\n\nlm_loss = LMLoss()\nloss = lm_loss(logits, yb)\nloss\n\ntensor(10.8916, grad_fn=&lt;NllLossBackward0&gt;)"
  }
]